{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t7BewNqgv8v"
      },
      "source": [
        "# Policy Gradients Methods\n",
        "\n",
        "(Google Notebook created by me inspired by courses on hugginface)\n",
        "\n",
        "\n",
        "In this notebook we will implement the reinforce algorithm from scratch, then train and evaluate it the gymnasium environment ‚ÄúLunarLander-v2‚Äù.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P22WUpshO5kl"
      },
      "source": [
        "### Installing and importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlU-KDAevetf",
        "outputId": "c62024d7-268e-4441-b7de-d0dfa1c762fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Waiting for headers] [Con\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Waiting for headers] [2 InRelease 14.2 kB/110 kB 13%] [Connecting to ppa.la\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [2 InRelease 14.2 kB/110 kB 13%] [Connecting to ppa.la\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [2 InRelease 30.1 kB/110 kB 27%] [Connecting to ppa.la\r                                                                               \rGet:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 229 kB in 1s (236 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1.post0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!apt install swig cmake\n",
        "!pip3 install gymnasium[box2d] swig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0ehkrD8xfm1"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from IPython.display import Image\n",
        "import torch\n",
        "torch.manual_seed(0) # set random seed\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import gymnasium as gym\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euSSRI1JhNDW"
      },
      "source": [
        "### Visualizing üê™\n",
        "\n",
        "In Google Colab, rendering Gym environments with graphical animations can be a bit tricky due to the limitations of the Colab environment. Therefore we're are rather saving and downloading a .mp4 file of the environment simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJZXPNR7ztob"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers import RecordVideo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVeD70NxPpHg"
      },
      "source": [
        "## The enviorment üéÆ\n",
        "\n",
        "The goal is to guide the lander (rocket) to land softly on the designated landing pad while managing limited fuel resources. The agent receives rewards or penalties based on its actions and the success of the landing\n",
        "\n",
        "Rewards in the enviornment:\n",
        "\n",
        "- is increased/decreased the closer/further the lander is to the landing pad.\n",
        "- is increased/decreased the slower/faster the lander is moving\n",
        "- is decreased the more the lander is tilted (angle not horizontal).\n",
        "- is increased by 10 points for each leg that is in contact with the ground.\n",
        "- is decreased by 0.03 points each frame a side engine is firing.\n",
        "- is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "Read more about the environment here: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "S9S1uR_RRILD",
        "outputId": "b1a7f8ca-7891-49ff-f84a-3cbefbaddd0e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://gymnasium.farama.org/_images/lunar_lander.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://gymnasium.farama.org/_images/lunar_lander.gif')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiwDMYgHqHxc"
      },
      "source": [
        "### Observation and Action Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJAcVUjoP-Fc",
        "outputId": "77ebabd5-8fb2-4034-8063-57277e7fc42a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation space: 8, action space: 4\n",
            "Observation [-0.00411797  1.4216837  -0.41714048  0.47836992  0.00477868  0.09448856\n",
            "  0.          0.        ]\n",
            "\n",
            "Random sampling of action:  0\n"
          ]
        }
      ],
      "source": [
        "test_env = gym.make(\"LunarLander-v2\", render_mode='rgb_array') # Create our environment called LunarLander-v2\n",
        "\n",
        "\n",
        "observation_space = test_env.observation_space\n",
        "action_space = test_env.action_space # Four discrete actions;  0 = do nothing, 1 = fire left orientation engine, 2 = fire main engine, 3 = fire right orientation engine\n",
        "\n",
        "observation, info = test_env.reset()\n",
        "\n",
        "print(f'Observation space: {observation_space.shape[0]}, action space: {action_space.n}')\n",
        "print(f'Observation {observation}') # 8-dimensional vector of with info about the state\n",
        "\n",
        "print(\"\\nRandom sampling of action: \", action_space.sample())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weaVL9-vpSSz"
      },
      "source": [
        "### Test of random action and video\n",
        "After running the cell below, you should be able to download a video of the episode from the folder \"lunar_videos/random/\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsjOPnJpvnQv",
        "outputId": "7bb0315a-b5d3-45a9-ad7a-7a4321bb6dd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/lunar_videos/random folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Building video /content/lunar_videos/random/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/lunar_videos/random/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/lunar_videos/random/rl-video-episode-0.mp4\n",
            "reset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "test_env = RecordVideo(test_env, './lunar_videos/random/') # Wrap test_env for video replay\n",
        "\n",
        "# Then we reset this environment\n",
        "observation, info = test_env.reset()\n",
        "for _ in range(1000):\n",
        "    action = test_env.action_space.sample()   # Take a random action\n",
        "\n",
        "    observation, reward, terminated, truncated, info = test_env.step(action) # Perform the action in the environment\n",
        "\n",
        "    # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
        "    if terminated or truncated:\n",
        "        print(\"reset\")\n",
        "        break\n",
        "test_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkVdSg3ds339"
      },
      "source": [
        "# Reinforce Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irQftNevG8jH",
        "outputId": "38e3bcdc-1c44-433f-acf6-c63afea69450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkcp1pzzsqvZ"
      },
      "source": [
        "## Policy\n",
        "\n",
        "Given the state (8 dim vector) we want to create a nerual network to output a probability distribution over the 4 possible actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxXWbw1n8pK5"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "  def __init__(self, state_size, action_size, hidden_size):\n",
        "    super().__init__()\n",
        "    # SOLUTION\n",
        "    self.f1 = nn.Linear(state_size, hidden_size)\n",
        "    self.f2 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # SOLUTION\n",
        "    x = F.relu(self.f1(x))\n",
        "    x = F.softmax(self.f2(x), dim=0) #  => softmax over cols ([ 0.1385, -0.0157,  0.1634, -0.2826])\n",
        "    return x\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.from_numpy(state).float().to(device)\n",
        "    probs = self.forward(state)\n",
        "    m = Categorical(probs) # makes it possible to use log_prob to implement REINFROCE (differentiable probability distribution)\n",
        "    action = m.sample() # sample from the distribution\n",
        "    return action.item(), m.log_prob(action) # Return action and log probability of the action, in order to do the gradient ascent(descent) later\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Mig-yivWE-"
      },
      "source": [
        "### Test the policy network with a forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQDifrl7GUaK",
        "outputId": "4a7483cf-fe52-499b-9caf-73661a5ccc39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.1074066e-03  1.4124206e+00 -1.1218278e-01  6.6686034e-02\n",
            "  1.2900012e-03  2.5411118e-02  0.0000000e+00  0.0000000e+00]\n",
            "self tensor([ 0.1385, -0.0157,  0.1634, -0.2826], device='cuda:0',\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(3, tensor(-1.6848, device='cuda:0', grad_fn=<SqueezeBackward1>))"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "observation, infor = env.reset(seed=3)\n",
        "policy = Policy(state_size, action_size, hidden_size=16).to(device)\n",
        "\n",
        "print(observation)\n",
        "policy.select_action(observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qo320oYwFg_"
      },
      "source": [
        "## Reinfroce Algorithm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://i.stack.imgur.com/D0K5F.png' />\n",
        "<figcaption>Pseudocode of Reinforce</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "Short recap of Reinforce Algo: *do more of the good actions and less the bad actions.*\n",
        "\n",
        "This is done by multiplying the graident of the log probability of selecting action **a_t** in state **s_t** with the sum of discounted rewards (G_t) from time **t** (line 7).\n",
        "\n",
        "\n",
        "Intuitively, if the action leads to a postive discounted cumulative reward, we would take a greater step in the direction of increasing the probability for this action by doing the mentioned multiplication. Likewise the opposite if negative reward.\n",
        "\n",
        "Tips: We only want to \"reinforce\" the action with rewards cumulated after taking *that* action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSAl65sOHjV6"
      },
      "outputs": [],
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every=100):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    policy (Policy): Our policy model\n",
        "    optimizer (torch.optim): To perform backpropagation and update step\n",
        "    n_training_episodes (int): Number of episodes during training\n",
        "    max_t (int): Max amount of steps in one episode\n",
        "    gamma (int): Discount factor of future reward\n",
        "    print_every (int)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  scores = []\n",
        "  for episode in range(0, n_training_episodes):\n",
        "    rewards = []\n",
        "    log_probs = []\n",
        "\n",
        "    observation, _ = env.reset()\n",
        "\n",
        "    # Line 4\n",
        "    for i in range(max_t): # until max timestap, terminated (completed/failure) or truncated\n",
        "      action, log_prob = policy.select_action(observation) # SOLUTION select an action from the policy\n",
        "      observation, reward, terminated, truncated, info = env.step(action) # take an action\n",
        "      rewards.append(reward) # append rewards\n",
        "      log_probs.append(log_prob)\n",
        "      env.render()\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "    scores.append(sum(rewards))\n",
        "    discounted_rewards = deque(maxlen=max_t) # deque with max length (list possible to append in both ends)\n",
        "    discounted_reward = 0\n",
        "    # Line 5\n",
        "    for i in range(len(rewards)):\n",
        "      discounted_reward = rewards[-i-1] + gamma*discounted_reward # SOLUTION N G_t = r_(t+1) + gammma*G_(t+1)\n",
        "      discounted_rewards.appendleft(discounted_reward) # SOLTUION  append left [G_T] -> [G_(T-1), G_T]\n",
        "\n",
        "    # Line 7\n",
        "    objective = []\n",
        "    for log_prob, discounted_reward in zip(log_probs, discounted_rewards):\n",
        "      objective.append(-log_prob * discounted_reward) #\n",
        "    objective1 = torch.tensor(objective,requires_grad=True).sum()¬®\n",
        "\n",
        "    objective.backward()\n",
        "    optimizer.step()\n",
        "    del rewards\n",
        "    del log_probs\n",
        "\n",
        "    if episode % print_every == 0:\n",
        "      print(f'Episode {episode}\\t Score: {scores[episode]}')\n",
        "\n",
        "  #torch.save(policy.state_dict(), './models/reinforce_model.pt')\n",
        "\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOv5aZSaM1MO"
      },
      "source": [
        "For tips: https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTHIxMLULmZQ"
      },
      "outputs": [],
      "source": [
        "policy = Policy(state_size, action_size, hidden_size=16).to(device)\n",
        "n_training_episodes = 1\n",
        "max_t = 1000\n",
        "gamma = 0.9\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52qfql6XLPNG"
      },
      "outputs": [],
      "source": [
        "scores = reinforce(policy, optimizer, n_training_episodes, max_t, gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4d_9mp_M942"
      },
      "source": [
        "# Get a video file of the trained policy üì∑"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSGbepT5KW6E"
      },
      "outputs": [],
      "source": [
        "env_eval = gym.make(\"LunarLander-v2\", render_mode='rgb_array')\n",
        "env_eval = RecordVideo(env_eval, './lunar_videos/reinforce/')\n",
        "\n",
        "observation, _ = env_eval.reset()\n",
        "for i in range(1000):\n",
        "    action, _state = policy.select_action(observation)\n",
        "    observation, reward, terminated, truncated, inf = env_eval.step(action)\n",
        "    env_eval.render()\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "env_eval.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
